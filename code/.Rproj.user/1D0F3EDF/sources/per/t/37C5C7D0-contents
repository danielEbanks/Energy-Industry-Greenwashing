##
# This script times Gibbs 
##


library(ggfortify)

## Joint Topic-Sentiment Modeling 

rm(list=ls())

# Load  NLP Libraries 

library(tidytext)
library(quanteda)
library(rJST)
library(stm)
library(text2vec)
# Utilities 
library(dplyr)
library(parallel)
library(ggplot2)
library(lubridate)

#cluster analysis 
library(factoextra)
library(fpc)


# Set Path
if(Sys.info()[1]!="Windows"){
  setwd("/home/debanks/Dropbox/Tensor_SVD/TensorJST/")
} else {
  setwd("/Users/danie/Dropbox/Tensor_SVD/TensorJST/")
}


textData <- read.csv("Data/TwitterSpeech.csv")
textData <- textData[!duplicated(textData$doc_id),]
textData <- textData[textData$year>=2019,]


# create a week
#textData$date <- as.POSIXct(
#  paste0(textData$created_at)
textData$week   <- isoweek(textData$date)


## prepare JST model
attach(textData)
llDisplay <- data.frame(doc_id = paste(doc_id),text = paste(tweet),week=week,handle=handle,month=month,day=day,year=year,party=party,state=state,stringsAsFactors = F)
detach(textData)

#n_samples = 300000
#llDisplay <- llDisplay[1:n_samples,]



stop_words_custom = c("amendment","family","get","adam","hear","feder","de","la","los","democrat","republican",
                      'el', 'para', 'en', 'que',"lo","RT","&","amp",
                      "amend","back","protect","commun","service","work","around","alway","november","august","january",
                      "happen","ive","hall","nation","work","service","this","discuss","community","learn","congressional","amendment","speaker","say","said","talk","congrats","pelosi","gop","congratulations","are","as","i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours","he","her","him","she","hers","that","be","with","their","they're","is","was","been","not","they","it","have","will","has","by","for","madam","Speaker","Mister","Gentleman","Gentlewoman","lady","voinovich","kayla","111th","115th","114th","rodgers",         "clerk" ,    "honor" ,   "address"   ,     
                      "house" , "start"   ,"amend","bipartisan","bill",   "114th"    ,   "congress"  ,     
                      "one",   "thing"    ,"bring","put", "north","give","keep","pa","even","texa","year","join","well",
                      "call",  "learned"    ,   "legislator","things" ,"things","can't","can","cant","will","go","going","let",
                      "lets","let's","say","says","know","talk","talked","talks","lady","honorable","dont","think","said","something",
                      "something","wont","people","make","want","went","goes","congressmen","people","person","like","come","from",
                      "need","us",paste(textData$firstname),paste(tidytext::stop_words$word),stopwords())
# Create tokens for the words you keep
toks1 <- tokens(corpus(llDisplay), remove_punct = TRUE)
toks2 <- tokens_remove(toks1, stop_words_custom)
toks3 <- tokens_ngrams(toks2, 1:3)
toks3 <- tokens_wordstem(toks3)



N = nrow(llDisplay)

# create a document feature matrix (quanteda style) to run JST
dfm_speeches <- dfm(toks3 , 
                    remove_punct = TRUE ,
                    remove_numbers = TRUE,
                    stem = T) %>% 
  dfm_trim(min_termfreq = 25,
           min_docfreq =0.002*N,
           max_docfreq = 0.4*N)


## Create Paradigm list
df_p       = read.csv("../Data/paradigm.csv")



## top 100 sentiments
top_sents =df_p[ df_p$Token %in% names(topfeatures(dfm_speeches,n=410)), ]

neutral_features = names(topfeatures(dfm_speeches,n=10000))[!(dfm_speeches@Dimnames$features
                                                              %in%top_sents$Token)]

paradigm_dict =dictionary(list(positive = top_sents$Token[top_sents$Positive==0.9],
                               negative = top_sents$Token[top_sents$Negative==0.9]
))

#optimize for topic number
kk = c(5,10,15,20,25,28,30,40,50,60)


print(Sys.time())
results <- mclapply(kk,
                    FUN = function(kk) 
                      jst(dfm_speeches,
                          paradigm_dict,
                          numTopics = kk,
                          numSentiLabs = 3,
                          numIters = 1000),
                    mc.cores = getOption("mc.cores", 
                                         10L))  
print(Sys.time())

save(results,file="Data/modelOutput3senti-Clim-1000.RData")


start= Sys.time()
jst(dfm_speeches,
    paradigm_dict,
    numTopics = 28,
    numSentiLabs = 3,
    numIters = 1000)
end=Sys.time()



## Coherence

dtm <- dfm(dfm_speeches,verbose = F)
tcm <- Matrix::crossprod(sign(dtm))


top20word_LDA        <- read.csv("Data/theta_LDA_TopWords.csv")
top20word_JST_Tensor <- read.csv("Data/theta_JST_TopWords.csv")

coherence_LDA <- colMeans(coherence(as.matrix(t(top20word_LDA)[2:11,]), tcm, metrics = c("mean_logratio", "mean_pmi", "mean_npmi",
                                                                               "mean_difference", "mean_npmi_cosim", "mean_npmi_cosim2"),
                                    smooth = 1e-12, n_doc_tcm = N),na.rm=T)

coherence_JST <- colMeans(coherence(as.matrix(top20words(results[[6]])), tcm, metrics = c("mean_logratio", "mean_pmi", "mean_npmi",
                                                                               "mean_difference", "mean_npmi_cosim", "mean_npmi_cosim2"),
                                    smooth = 1e-12, n_doc_tcm = N),na.rm=T)

coherence_JST_Tensor <- colMeans(coherence(as.matrix(t(top20word_JST_Tensor)[2:11,]), tcm, metrics = c("mean_logratio", "mean_pmi", "mean_npmi",
                                                                               "mean_difference", "mean_npmi_cosim", "mean_npmi_cosim2"),
                                    smooth = 1e-12, n_doc_tcm = N),na.rm=T)



result_co = sapply(seq(1:10),FUN=function(x) colMeans(coherence(as.matrix(top20words(results[[x]])), tcm, metrics = c("mean_logratio", "mean_pmi", "mean_npmi",
                                                                                                          "mean_difference", "mean_npmi_cosim", "mean_npmi_cosim2"),
                                                    smooth = 1e-12, n_doc_tcm = N),na.rm=T))

result_co = result_co[5,]
## PCA/Stategy


## Emblematic Tweets


## Topical Evolution


